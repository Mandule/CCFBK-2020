{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.3.1\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "logging.basicConfig(level=logging.ERROR)\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from tqdm import tqdm\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras.backend as K\n",
    "import os\n",
    "from transformers import *\n",
    "print(tf.__version__)\n",
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 参数下载地址 https://huggingface.co/bert-base-chinese"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_left = pd.read_csv('./train/train.query.tsv',sep='\\t',header=None)\n",
    "train_left.columns=['id','q1']\n",
    "train_right = pd.read_csv('./train/train.reply.tsv',sep='\\t',header=None)\n",
    "train_right.columns=['id','id_sub','q2','label']\n",
    "df_train = train_left.merge(train_right, how='left')\n",
    "df_train['q2'] = df_train['q2'].fillna('好的')\n",
    "test_left = pd.read_csv('./test/test.query.tsv',sep='\\t',header=None, encoding='gbk')\n",
    "test_left.columns = ['id','q1']\n",
    "test_right =  pd.read_csv('./test/test.reply.tsv',sep='\\t',header=None, encoding='gbk')\n",
    "test_right.columns=['id','id_sub','q2']\n",
    "df_test = test_left.merge(test_right, how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train shape = (21585, 5)\n",
      "test shape = (53757, 4)\n"
     ]
    }
   ],
   "source": [
    "PATH = './'\n",
    "BERT_PATH = './'\n",
    "WEIGHT_PATH = './'\n",
    "MAX_SEQUENCE_LENGTH = 100\n",
    "input_categories = ['q1','q2']\n",
    "output_categories = 'label'\n",
    "\n",
    "print('train shape =', df_train.shape)\n",
    "print('test shape =', df_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _convert_to_transformer_inputs(question, answer, tokenizer, max_sequence_length):\n",
    "    \"\"\"Converts tokenized input to ids, masks and segments for transformer (including bert)\"\"\"\n",
    "    \n",
    "    def return_id(str1, str2, truncation_strategy, length):\n",
    "\n",
    "        inputs = tokenizer.encode_plus(str1, str2,\n",
    "            add_special_tokens=True,\n",
    "            max_length=length,\n",
    "            truncation_strategy=truncation_strategy,\n",
    "            #truncation=True\n",
    "            )\n",
    "        \n",
    "        input_ids =  inputs[\"input_ids\"]\n",
    "        input_masks = [1] * len(input_ids)\n",
    "        input_segments = inputs[\"token_type_ids\"]\n",
    "        padding_length = length - len(input_ids)\n",
    "        padding_id = tokenizer.pad_token_id\n",
    "        input_ids = input_ids + ([padding_id] * padding_length)\n",
    "        input_masks = input_masks + ([0] * padding_length)\n",
    "        input_segments = input_segments + ([0] * padding_length)\n",
    "        \n",
    "        return [input_ids, input_masks, input_segments]\n",
    "    \n",
    "    input_ids_q, input_masks_q, input_segments_q = return_id(\n",
    "        question, answer, 'longest_first', max_sequence_length)\n",
    "    \n",
    "    return [input_ids_q, input_masks_q, input_segments_q]\n",
    "\n",
    "def compute_input_arrays(df, columns, tokenizer, max_sequence_length):\n",
    "    input_ids_q, input_masks_q, input_segments_q = [], [], []\n",
    "    input_ids_a, input_masks_a, input_segments_a = [], [], []\n",
    "    for _, instance in tqdm(df[columns].iterrows()):\n",
    "        q, a = instance.q1, instance.q2\n",
    "\n",
    "        ids_q, masks_q, segments_q= \\\n",
    "        _convert_to_transformer_inputs(q, a, tokenizer, max_sequence_length)\n",
    "        \n",
    "        input_ids_q.append(ids_q)\n",
    "        input_masks_q.append(masks_q)\n",
    "        input_segments_q.append(segments_q)\n",
    "\n",
    "    return [np.asarray(input_ids_q, dtype=np.int32),\n",
    "            np.asarray(input_masks_q, dtype=np.int32),\n",
    "            np.asarray(input_segments_q, dtype=np.int32)]\n",
    "\n",
    "def compute_output_arrays(df, columns):\n",
    "    return np.asarray(df[columns])\n",
    "\n",
    "def search_f1(y_true, y_pred):\n",
    "    best = 0\n",
    "    best_t = 0\n",
    "    for i in range(30,60):\n",
    "        tres = i / 100\n",
    "        y_pred_bin =  (y_pred > tres).astype(int)\n",
    "        score = f1_score(y_true, y_pred_bin)\n",
    "        if score > best:\n",
    "            best = score\n",
    "            best_t = tres\n",
    "    print('best', best)\n",
    "    print('thres', best_t)\n",
    "    return best, best_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1015 07:57:39.324805 140024237856576 tokenization_utils.py:929] Model name './bert-base-chinese-vocab.txt' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, bert-base-finnish-cased-v1, bert-base-finnish-uncased-v1, bert-base-dutch-cased). Assuming './bert-base-chinese-vocab.txt' is a path, a model identifier, or url to a directory containing tokenizer files.\n",
      "W1015 07:57:39.328269 140024237856576 tokenization_utils.py:941] Calling BertTokenizer.from_pretrained() with the path to a single file or url is deprecated\n",
      "I1015 07:57:39.329980 140024237856576 tokenization_utils.py:1013] loading file ./bert-base-chinese-vocab.txt\n",
      "21585it [00:09, 2241.64it/s]\n",
      "53757it [00:23, 2331.12it/s]\n"
     ]
    }
   ],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained(BERT_PATH+'bert-base-chinese-vocab.txt')\n",
    "outputs = compute_output_arrays(df_train, output_categories)\n",
    "inputs = compute_input_arrays(df_train, input_categories, tokenizer, MAX_SEQUENCE_LENGTH)\n",
    "test_inputs = compute_input_arrays(df_test, input_categories, tokenizer, MAX_SEQUENCE_LENGTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model():\n",
    "    q_id = tf.keras.layers.Input((MAX_SEQUENCE_LENGTH,), dtype=tf.int32)\n",
    "    q_mask = tf.keras.layers.Input((MAX_SEQUENCE_LENGTH,), dtype=tf.int32)\n",
    "    q_atn = tf.keras.layers.Input((MAX_SEQUENCE_LENGTH,), dtype=tf.int32)\n",
    "    \n",
    "    config = BertConfig.from_pretrained('./bert-base-chinese-config.json') \n",
    "    config.output_hidden_states = False \n",
    "    bert_model = TFBertModel.from_pretrained('./bert-base-chinese-tf_model.h5', \n",
    "                                             config=config)\n",
    "    q_embedding = bert_model(q_id, attention_mask=q_mask, token_type_ids=q_atn)[0]\n",
    "    q = tf.keras.layers.GlobalAveragePooling1D()(q_embedding)\n",
    "    a = tf.keras.layers.GlobalMaxPooling1D()(q_embedding)\n",
    "    t = q_embedding[:,-1]\n",
    "    e = q_embedding[:, 0]\n",
    "    x = tf.keras.layers.Concatenate()([q, a, t, e])\n",
    "    \n",
    "    x = tf.keras.layers.Dropout(0.5)(x)\n",
    "    x = tf.keras.layers.Dense(1, activation='sigmoid')(x)\n",
    "    model = tf.keras.models.Model(inputs=[q_id, q_mask, q_atn], outputs=x)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GroupKFold\n",
    "\n",
    "gkf = GroupKFold(n_splits=5).split(X=df_train.q2, groups=df_train.id)\n",
    "\n",
    "valid_preds = []\n",
    "test_preds = []\n",
    "\n",
    "oof = np.zeros((len(df_train),1))\n",
    "for fold, (train_idx, valid_idx) in enumerate(gkf):\n",
    "    train_inputs = [inputs[i][train_idx] for i in range(len(inputs))]\n",
    "    train_outputs = outputs[train_idx]\n",
    "    valid_inputs = [inputs[i][valid_idx] for i in range(len(inputs))]\n",
    "    valid_outputs = outputs[valid_idx]\n",
    "    \n",
    "    K.clear_session()\n",
    "    model = create_model()\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=2e-5)\n",
    "    model.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=[tf.keras.metrics.AUC()])\n",
    "    model.fit(train_inputs, train_outputs, validation_data = (valid_inputs, valid_outputs), epochs=3, batch_size=64)\n",
    "    oof_p = model.predict(valid_inputs, batch_size=512)\n",
    "    oof[valid_idx] = oof_p\n",
    "    valid_preds.append(oof_p)\n",
    "    test_preds.append(model.predict(test_inputs, batch_size=512))\n",
    "    f1,t = search_f1(valid_outputs, valid_preds[-1])\n",
    "    print('validation score = ', f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best 0.7719488011537767\n",
      "thres 0.4\n"
     ]
    }
   ],
   "source": [
    "best_score, best_t = search_f1(outputs,oof)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub = np.average(test_preds, axis=0) \n",
    "sub = sub > best_t\n",
    "df_test['label'] = sub.astype(int)\n",
    "df_test[['id','id_sub','label']].to_csv('submission_beike_{}.csv'.format(best_score),index=False, header=None,sep='\\t')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_tensorflow2_py3",
   "language": "python",
   "name": "conda_tensorflow2_py3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
